{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import EnglishStemmer  # Assuming we're working with English\n",
    "import string \n",
    "\n",
    "\n",
    "class Index:\n",
    "    \"\"\" Inverted index datastructure \"\"\"\n",
    " \n",
    "    def __init__(self, tokenizer, stemmer=None, stopwords=None):\n",
    "        \"\"\"\n",
    "        tokenizer   -- NLTK compatible tokenizer function\n",
    "        stemmer     -- NLTK compatible stemmer \n",
    "        stopwords   -- list of ignored words\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.index = defaultdict(list)\n",
    "        self.docindexes = {}\n",
    "        self.documents = {}\n",
    "        self.__unique_id = 0\n",
    "        if not stopwords:\n",
    "            self.stopwords = set()\n",
    "        else:\n",
    "            self.stopwords = set(stopwords)\n",
    " #The stopwords list is used so that the index doesn’t create an entry for every word in the English language. \n",
    " #The words contained in such lists have ideally no semantics by their own(so, that, the,…).\n",
    "#The stemmer is used to get a common form for different inflections of the base word (watching -> watch, ghostly -> ghost, etc…). \n",
    "#The stem of the word is not necessarily a dictionary word. Stemmers use heuristic approaches for determining the base form of the word fast\n",
    "#Ex: looking for \"centrality\" would be in fact looking for \"central\"   \n",
    "   \n",
    "    def word_counts(self, text, word):\n",
    "        \"\"\"Return a vector that represents the counts of specific words in the text\n",
    "        word_counts(\"Here is sentence one. Here is sentence two.\", ['Here', 'two', 'three'])\n",
    "        [2, 1, 0]\n",
    "        emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "        word_counts(emma, ['the', 'a'])\n",
    "        [4842, 3001]\n",
    "        \"\"\"  \n",
    "\n",
    "        words = nltk.word_tokenize(text) \n",
    "        counts =  nltk.FreqDist(words)   # this counts all word occurences\n",
    "        return str(counts[word])\n",
    "        #return counts[word]\n",
    "        #return [counts[x] or 0 for x in words] # this returns what was counted for *words\n",
    "\n",
    "    \n",
    "    def lookup(self, word):\n",
    "        \"\"\"\n",
    "        Lookup a word in the index\n",
    "        \"\"\"\n",
    "        word = word.lower()\n",
    "        #if self.stemmer:\n",
    "            #word = self.stemmer.stem(word)\n",
    "           \n",
    "        result=[]\n",
    "        \n",
    "        for id in self.index.get(word):\n",
    "            count=self.word_counts(self.documents.get(id, None), word)\n",
    "            result.append(tuple((id, count)))\n",
    "            \n",
    "        return result\n",
    "        \n",
    "        #self.documents.get(id, None) -  returns the document with id \"id\"\n",
    "        #self.index.get(word) - returns list of the ids of the documents with that word    \n",
    "        \n",
    "    def add(self, document):\n",
    "        \"\"\"\n",
    "        Add a document string to the index\n",
    "        \"\"\"\n",
    "        for token in [t.lower() for t in nltk.word_tokenize(document)]:\n",
    "            if token in self.stopwords:\n",
    "                continue\n",
    " \n",
    "            #if self.stemmer:\n",
    "                #token = self.stemmer.stem(token)\n",
    "        #Add this word to this index\n",
    "            if self.__unique_id not in self.index[token]:\n",
    "                self.index[token].append(self.__unique_id)\n",
    "                \n",
    "        #Add this document to a list in this index\n",
    "        self.documents[self.__unique_id] = document\n",
    "        self.__unique_id += 1    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centrality exists in the documents x with frequency y: (x,y)\n",
      "[(0, '1'), (1, '3'), (2, '1')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = Index(nltk.word_tokenize, EnglishStemmer(), nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "filename = 'file.txt'\n",
    "file_to_read = open(filename)\n",
    "lines = [line.rstrip('\\n') for line in file_to_read]\n",
    "documents=lines\n",
    "\n",
    "for document_key in documents:\n",
    "    with open(document_key, 'r') as file:\n",
    "        file_to_index=file.read().replace('\\n', '')\n",
    "        index.add(file_to_index)    \n",
    "\n",
    "print(\"Centrality exists in the documents x with frequency y: (x,y)\")\n",
    "print(index.lookup(\"centrality\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Duvidas: Temos de cosntruir a contagem do indice logo à priori ou só quao pedem certas palavras?\"\n",
    "\"Se nao, entao como melhoramos aquela ineficiencia do word_counting?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
